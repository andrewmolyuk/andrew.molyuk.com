---
title: "Поднимаем реплики MongoDB в AWS на Docker"
date: 2023-06-20T22:38:35+03:00
blog/tags: [ "mongodb", "aws", "replica set", "docker" ]
cover:
  image: "IBM_1011_and_IBM_1418_as_IBM_1460_accessories_(1).jpg"
  title: "IBM 1011 and IBM 1418 as IBM 1460 accessories"
  link: "https://commons.wikimedia.org/wiki/File:IBM_1011_and_IBM_1418_as_IBM_1460_accessories_(1).jpg"
draft: false
---

У нас в компании есть внутреннее приложение, которое использует MongoDB. Приложение состоит из множества микросервисов,
каждый из которых взаимодействует со своим собственным экземпляром MongoDB. Сильной нагрузки на приложение нет, но оно
критично для бизнеса, а потому нам нужно, чтобы оно было отказоустойчивым. Создавать несколько экземпляров MongoDB для
каждого сервиса показалось не целесообразным и дорогостоящим решением. Поэтому мы решили использовать единый
отказоустойчивый кластер MongoDB. В шардинге у нас пока еще нет необходимости, поэтому мы будем использовать только
реплики MongoDB. В этой статье я опишу, как я поднял реплики в AWS на Docker и как мы их используем в нашем приложении.

<!--more-->

## База данных и контейнеризация всей страны

Для начала я скажу пару слов о нашей инфраструктуре и о мотивации использовать Docker для баз данных.

В нашей компании есть сервисы, которые используют MongoDB. Все они разворачиваются в AWS и логически разделены по ЕС2
машинам и зонам ответственности. Все сервисы бегут в контейнерах, как и базы данных. Все это дело практически не
управляется. Поэтому мы решили начать с разделения всего хозяйства на две категории по типу работы с данными. Первая
категория - это базы данных, устойчивые очереди и прочие сервисы, которые работают с диском. Вторая категория - это
сервисы, которые эти данные только обрабатывают и не хранят ничего связанного с состоянием системы. Таким образом мы
получили стейтфул и стейтлесс категории. Все сервисы в категории стейтлесс могут быть запущены в любом месте, где есть
ресурсы. Сервисы в категории стейтфул должны быть запущены в одном месте, чтобы не было проблем с доступом к данным.

Естественно, что базы данных должны быть в категории стейтфул. И естественно, что сразу возникает вопрос о том, а зачем
нам Docker для баз данных, тем более что на эту тему идет постоянная священная война в сообществе. Но мы не будем об
этом сейчас.

Мы решили использовать Docker для баз данных. Во-первых, это красиво. Во-вторых, это позволяет нам использовать один и
тот же подход для всех сервисов, а следовательно разработка и поддержка становится проще. В-третьих, это позволяет нам
использовать идентичную среду для разработки и для боевых сервисов в AWS. Естественно, что в самих контейнерах мы не
будем хранить данные, а будем использовать тома подключенные к контейнерам.

В случае проблем с контейнером достаточно перезапустить его без потери работоспособности кластера и доступа к данным. В
случае проблем с томом или самим хостом мы можем просто вывести его из репликации и создать новый. В случае проблем с
зоной AWS мы можем поднять дополнительные реплики в работающих зонах и подключить их к кластеру. В случае проблем с AWS
мы ничего не можем сделать и проблемы тогда у всех в отрасли и работа с клиентом затруднена на всех уровнях, что можно
игнорировать в рамках нашего приложения.

## Что такое реплика MongoDB?

Немного теории. Много букв. Мало кода. Кому не интересно - листаем дальше.

Итак, мы решили использовать реплики MongoDB. Что это такое? Реплика - это набор экземпляров MongoDB, которые хранят
один и тот же набор данных. Один из экземпляров является основным, а остальные - вторичными. Запись данных происходит в
основную ноду и реплицируется во вторичные. В случае падения основной ноды, одна из вторичных нод становится основной и
работа продолжается. В случае падения вторичной ноды, она просто перезапускается и продолжает работу. В случае падения
всех нод, кластер не работает и требуется вмешательство человека. Но такое падение очень маловероятно, если все ноды
размещены в разных зонах AWS.

Чтобы не нагружать основную ноду, чтение можно поручить вторичным нодам. При этом, если вторичная нода не успевает за
основной, то она может отставать на несколько секунд. Это нормально и не должно вызывать проблем. В случае, если
вторичная нода отстает на несколько минут, то это уже проблема и требуется вмешательство человека. С такой ситуацией мы
не сталкивались и все данные в кластере реплицируются в пределах миллисекунд. Но важно понимать, что такая ситуация
возможна и данные поступают в кластер асинхронно.

Настройки чтения могут быть разными. Например, можно настроить чтение только с основной ноды, только с вторичных нод или
с предпочтением к основной ноде, а потом к вторичным. Также можно настроить чтение с ближайшей ноды по карте сети. Это
может быть полезно, если вторичные ноды находятся в разных зонах AWS и нужно минимизировать время ответа. Но в этом
случае нужно быть готовым к тому, что данные могут быть неактуальными.

![mongo-replica.png](mongo-replica.png)

Ноды слушают друг друга, и этот процесс называется `heartbeat`. То есть каждая нода постоянно проверяется другими на
предмет доступности, для того, чтобы предпринимать какие-то действия, если что-то случилось. В момент когда основная
нода падает, вторичная нода становится основной и начинает принимать записи. Для выбора основной ноды используется
алгоритм выбора приоритета. Нода с наибольшим приоритетом становится основной. Приоритет задается в конфигурации ноды и
может быть любым. Но это не значит, что нода с наименьшим приоритетом не может стать основной. Она может стать основной
в том случае, если нода с наибольшим приоритетом не доступна.

В некоторых случаях возможно добавление арбитра в реплику. Арбитр - это нода, которая не хранит данные, но принимает
активное участие в выборе основной ноды. Арбитр может быть полезен, если у нас есть 2 ноды в реплике и нет возможности
добавить третью по причине, например высокой стоимости. Но это не наш случай, поэтому мы не будем использовать арбитров.

Падение основной ноды может вызвать задержку в работе приложения на время пока не будет выбрана новая основная нода.
Поэтому важно, чтобы вторичные ноды находились также в зоне с основной. Таким образом, если основная нода падает, то
приложение может подключиться к вторичной ноде в той же зоне и продолжить работу без задержек. Это приводит нас к тому,
что в каждой зоне имеет смысл поднимать минимум по две ноды. И тогда не важно, какая нода падает, приложение может
подключиться к другой ноде в той же зоне и производительность не упадет.

Если есть необходимость в балансировке нагрузки, то можно реализовать это на уровне драйвера или же просто указать
`secondaryPreferred` в настройках чтения. Тогда чтение будет происходить с вторичных нод, если они доступны, и с
основной ноды, если вторичные ноды не доступны. Таким образом, нагрузка будет распределена между всеми нодами в кластере
по раунд-робин алгоритму. Мы же будем использовать чтение ближайшей ноды, так как вторичные ноды находятся в разных
зонах.

Основной минус реплик MongoDB - это асинхронность. Представим ситуацию, когда у нас есть основная нода и две вторичные.
При записи данных в основную ноду, она реплицируется во вторичные. Предположим, что одна из вторичных нод не успевает за
основной и отстает. В этом случае у нас может получиться запрос на чтение части данных из актуальной вторичной ноды, и
части данных из вторичной ноды, которая не актуальна. Это может привести к некорректной работе приложения. Эта ситуация
решается настройкой `write concern` в значение `majority`. Тогда запись будет считаться успешной только в том случае,
если она будет реплицирована в большинство нод. То есть в нашем случае запись будет считаться успешной только в том
случае, если она будет реплицирована в основную ноду и в одну из вторичных. Таким образом, чтение будет происходить
только из актуальной ноды. Это решает проблему асинхронности, но при этом увеличивает время записи, так как запись будет
считаться успешной только после репликации в большинство нод. Это настройка и поведение по умолчанию в MongoDB, но я бы
рекомендовал проверить ее в вашем случае.

## Поднимаем реплики в AWS

Что же, хватит теории и давайте перейдем к практике. Для начала поднимем реплики в AWS. Для этого нам понадобится как
минимум четыре машинки по две в разных зонах. Я буду использовать `m7g.medium` инстансы на ARM архитектуре с Amazon
Linux 2023. После того как мы поднимем машины, нужно будет установить Docker и MongoDB на них.

### Установка Docker

Для установки Docker на Amazon Linux 2023 выполните следующие действия:

- Обновите пакеты вашего инстанса с помощью команды: sudo yum update -y.
- Установите Docker, выполнив следующие команды:

```shell
sudo amazon-linux-extras install docker
sudo service docker start
sudo usermod -a -G docker ec2-user
```

- Перезапустите сеанс SSH, чтобы изменения вступили в силу.
- Проверьте, что Docker установлен правильно, выполнив команду: `docker info`.

### Установка MongoDB

Для установки MongoDB на Amazon Linux 2023 выполните следующие действия:

- Сначала загрузите официальный образ MongoDB из Docker Hub с помощью команды:

```shell
sudo docker pull mongo
```

- Создаем docker-compose.yml файл со следующим содержимым:

```yaml
version: '3.8'
services:
  mongo:
    container_name: mongo
    image: mongo:latest
    command: mongod --replSet RS --port 27017 --bind_ip_all --setParameter disableSplitHorizonIPCheck=true
    restart: unless-stopped
    ports:
      - 27017:27017
    volumes:
      - mongo_data:/data/db
      - /etc/localtime:/etc/localtime:ro
    networks:
      - mongo
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: "3"

networks:
  mongo:
    name: mongo
    driver: bridge

volumes:
  mongo_data:
    name: mongo_data
    driver: local
``` 

Обратите внимание на параметр `--setParameter disableSplitHorizonIPCheck=true`. Он необходим для того, чтобы MongoDB
смог работать с IP адресами, а не только с доменными именами. Это удобнее для нас, так как мы будем использовать IP
адреса в качестве адресов нод. Вы можете использовать доменные имена, если хотите и тогда этот параметр не нужен.

- Запускаем контейнер с помощью команды: `docker-compose up -d`.
- Проверяем, что контейнер запустился с помощью команды: `docker ps`.

Теперь MongoDB будет работать внутри контейнера Docker на вашем инстансе Amazon Linux 2023. Вы можете подключиться к
MongoDB, используя IP-адрес вашего инстанса и порт 27017.

Таким образом поднимаем четыре инстанса и устанавливаем на них Docker и MongoDB. Причем, настройка будет одинаковой для
всех нод. Этот процесс можно автоматизировать с помощью Ansible или Terraform, но я не буду этого сейчас делать. Как уже
мы разбирали ранее, для репликации нам нужно как минимум четыре ноды, по две в разных зонах. Я буду использовать
следующую конфигурацию:

| Нода   | Зона       | IP адрес  |
|--------|------------|-----------|
| mongo1 | us-east-1a | 10.0.11.1 |
| mongo2 | us-east-1b | 10.0.12.1 |
| mongo3 | us-east-1a | 10.0.11.2 |
| mongo4 | us-east-1b | 10.0.12.2 |

Таким образом, у нас будет две ноды в зоне `us-east-1a` и две ноды в зоне `us-east-1b`. Адреса нод я выбрал произвольно,
вы можете использовать любые другие адреса. Главное, чтобы они были в разных зонах.

### Настраиваем реплики

Теперь, когда у нас есть все 4 ноды, мы можем настроить реплики. Для этого нам нужно подключиться к одной из нод,
которая и станет `primary` нодой. Я буду использовать `mongo1` ноду. Для подключения к ней выполните следующую команду:

```shell
docker exec -it mongo mongosh
```

После этого вы попадете в интерактивную оболочку MongoDB. Теперь мы можем настроить реплики. Для этого выполните
следующие команды:

```shell
rs.initiate( {
   _id : "RS",
   members: [
      { _id: 0, host: "10.0.11.1:27017" },
      { _id: 1, host: "10.0.12.1:27017" },
      { _id: 2, host: "10.0.11.2:27017" },
      { _id: 3, host: "10.0.12.2:27017" }
   ]
})
```

После этого репликация будет настроена и вы можете проверить ее статус с помощью команды: `rs.status()`. Вы должны
увидеть большой json с параметрами настройки самой реплики, а также со статусом всех нод. Все ноды должны быть в
статусе `SECONDARY`, кроме одной, которая будет в статусе `PRIMARY`. В моем случае это `mongo1` нода.

## Подключаемся к отдельной реплике

Теперь, когда репликация настроена, мы можем подключиться к ней с нашей локальной машины. Для этого нам нужно открыть
приложение для подключения к MongoDB. Я буду использовать [Studio 3T](https://studio3t.com/), но вы можете использовать
любой другой клиент, который вам нравится.

Чтобы подключиться к реплике, нам нужно указать все ноды, которые входят в реплику. В нашем случае это не
совсем удобно, поэтому мы можем подключиться к одной из нод.

Подключаемся к `mongo1` ноде с помощью Studio 3T. Если мы подключены к AWS VPN, то мы можем подключиться к ноде напрямую
по ее IP адресу. В противном случае нам нужно будет подключиться либо через SSH туннель, либо через SSH прокси. Мы
используем SSM Agent для подключения к нашим инстансам, поэтому мы можем использовать SSH прокси. Для этого нам нужно
выполнить следующую команду:

```shell
aws ssm start-session --target i-0123456789abcsdef --document-name AWS-StartPortForwardingSessionToRemoteHost --parameters '{"portNumber":["27017"], "localPortNumber":["27017"]}'
```

AWS Session Manager это отдельная тема, которую я возможно рассмотрю в отдельной статье. Пока что просто запомните, что
мы можем подключиться к нашим инстансам через SSM Agent.

Таким образом мы можем подключиться к `mongo1` ноде через локальный порт `27017`. После этого мы можем подключиться к
ней с помощью Studio 3T.

Если мы хотим производить какие-либо операции записи в базу данных, то мы должны подключаться к `PRIMARY` ноде. В этом
случае мы можем подключиться к `mongo1` ноде, так как она является `PRIMARY` нодой. Если мы хотим производить только
операции чтения, то мы можем подключаться к любой ноде, так как все они являются `SECONDARY` нодами.

Для определения того, какая нода является `PRIMARY`, мы можем выполнить следующую команду:

```shell
rs.isMaster()
```

Как альтернативный вариант запуска этой команды и получения адреса `PRIMARY` ноды выполните следующую команду:

```shell
db.runCommand({"isMaster": 1})["primary"]
```

Мы также можем посмотреть на статус реплики с помощью команды `rs.status()` и определить кто является `PRIMARY` нодой.

## Подключаемся к реплике из приложения

Теперь, когда мы можем подключаться к реплике из нашей локальной машины, мы можем подключиться к ней из нашего
приложения.

Для этого нам нужно указать все ноды, которые входят в реплику. Можно настроить подключение к реплике с помощью
следующей строки подключения:

```shell
mongodb://10.0.11.1:27017,10.0.12.1:27017,10.0.11.2:27017,10.0.12.2:27017/database?replicaSet=RS&readPreference=nearest
```

В этом случае мы указываем все ноды, которые входят в реплику, а также указываем имя реплики `RS`. Также мы
указываем `readPreference=nearest`, чтобы наше приложение могло читать данные с ближайшей ноды. Это позволит нам не
делать запросы к `PRIMARY` ноде, если это не требуется и обращаться к `SECONDARY` нодам в той же зоне доступности, что и
наше приложение.

Для более подробной информации о строке подключения к MongoDB, вы можете
посмотреть [официальную документацию](https://docs.mongodb.com/manual/reference/connection-string/).

Ну и самое главное не забывайте про резервное копирование данных, так как репликация не является заменой резервного
копирования. Сделать это мы можем с любой `SECONDARY` ноды, чтобы не мешать работе `PRIMARY` ноды.

## Заключение

В этой статье мы рассмотрели как настроить репликацию MongoDB в AWS. Мы настроили репликацию в две зоны доступности по
две ноды в каждой зоне. Также мы рассмотрели как подключиться к реплике из нашего приложения. Я показал как можно это
сделать с помощью строки подключения, а также с помощью AWS SSM Agent с локальной машины. Наверняка многие аспекты
данной темы я не рассмотрел, но я надеюсь, что эта статья поможет вам настроить репликацию MongoDB в короткие сроки. В
любом случае, если у вас есть вопросы, то вы можете задать их в комментариях ниже.
